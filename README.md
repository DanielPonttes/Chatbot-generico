# ğŸ¤– Chatbot GenÃ©rico

> Um chatbot simples e **gratuito** para projetos de faculdade, com suporte a modelos LLM locais via Ollama ou HuggingFace Inference API.

[![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109%2B-009688.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Requisitos](#-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [Como Executar](#-como-executar)
- [Interface Web (Frontend)](#-interface-web-frontend)
- [Exemplos de Uso](#-exemplos-de-uso)
- [Estrutura de Pastas](#-estrutura-de-pastas)
- [API Reference](#-api-reference)
- [Testes](#-testes)
- [LimitaÃ§Ãµes](#-limitaÃ§Ãµes)
- [PrÃ³ximos Passos](#-prÃ³ximos-passos)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um chatbot de perguntas e respostas em portuguÃªs brasileiro, com:

- **MemÃ³ria de conversa**: MantÃ©m contexto das Ãºltimas 10 mensagens por sessÃ£o
- **Persona configurÃ¡vel**: Customize o comportamento do bot via variÃ¡vel de ambiente
- **MÃºltiplos providers**: Ollama (local, gratuito) ou HuggingFace (API, gratuito com limites)
- **API HTTP**: Pronto para integrar com qualquer frontend
- **Custo zero**: Projetado para rodar em notebooks comuns sem custos

### Por que Qwen 2.5 0.5B como modelo padrÃ£o?

| Modelo | ParÃ¢metros | Tamanho | Qualidade PT-BR | Velocidade |
|--------|-----------|---------|-----------------|------------|
| **qwen2.5:0.5b** â­ | 500M | ~400MB | Boa | Muito rÃ¡pido |
| qwen2.5:1.5b | 1.5B | ~1GB | Muito boa | RÃ¡pido |
| llama3.2:1b | 1B | ~700MB | Boa | RÃ¡pido |
| phi3:mini | 3.8B | ~2GB | Excelente | Moderado |

O **Qwen 2.5 0.5B** foi escolhido como padrÃ£o por ser o menor modelo que ainda entrega respostas aceitÃ¡veis em portuguÃªs, ideal para notebooks com recursos limitados.

## ğŸ“¦ Requisitos

- **Python 3.11+**
- **Ollama** (recomendado) ou **HuggingFace Token**
- ~1GB de espaÃ§o em disco (para o modelo)
- 4GB+ de RAM recomendado

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone <url-do-repositorio>
cd chatbot-generico
```

### 2. Crie um ambiente virtual

```bash
# Criar venv
python3 -m venv venv

# Ativar (Linux/Mac)
source venv/bin/activate

# Ativar (Windows)
.\venv\Scripts\activate
```

### 3. Instale as dependÃªncias

**OpÃ§Ã£o A: pip (mais simples)**
```bash
pip install -r requirements.txt
```

**OpÃ§Ã£o B: pip com extras de desenvolvimento**
```bash
pip install -e ".[dev]"
```

### 4. Configure as variÃ¡veis de ambiente

```bash
cp .env.example .env
# Edite o .env conforme necessÃ¡rio
```

## âš™ï¸ ConfiguraÃ§Ã£o

### Usando Ollama (Recomendado) ğŸ 

O Ollama permite executar modelos LLM localmente, sem custo e sem internet.

#### Passo 1: Instale o Ollama

```bash
# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Mac
brew install ollama

# Windows
# Baixe o instalador em: https://ollama.ai/download
```

#### Passo 2: Inicie o servidor Ollama

```bash
ollama serve
```

#### Passo 3: Baixe o modelo

```bash
# Modelo padrÃ£o (recomendado, ~400MB)
ollama pull qwen2.5:0.5b

# Alternativas (melhor qualidade, mais pesados):
# ollama pull qwen2.5:1.5b   # ~1GB
# ollama pull llama3.2:1b    # ~700MB
# ollama pull phi3:mini      # ~2GB
# ollama pull deepseek-r1:latest # ~5GB
```

#### Passo 4: Configure o .env

```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:0.5b
```

### Usando HuggingFace (Alternativa) ğŸŒ

Se nÃ£o puder instalar Ollama, use a API do HuggingFace como fallback.

#### Passo 1: Obtenha um token

1. Acesse [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Clique em "New token"
3. DÃª um nome e copie o token

#### Passo 2: Configure o .env

```env
LLM_PROVIDER=huggingface
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
HF_MODEL=microsoft/DialoGPT-small
```

> âš ï¸ **AtenÃ§Ã£o**: A camada gratuita do HuggingFace tem limites de requisiÃ§Ãµes. Para uso intensivo, prefira Ollama.

### VariÃ¡veis de Ambiente DisponÃ­veis

| VariÃ¡vel | DescriÃ§Ã£o | PadrÃ£o |
|----------|-----------|--------|
| `LLM_PROVIDER` | Provider a usar: `ollama` ou `huggingface` | `ollama` |
| `OLLAMA_BASE_URL` | URL do servidor Ollama | `http://localhost:11434` |
| `OLLAMA_MODEL` | Modelo Ollama | `qwen2.5:0.5b` |
| `HF_TOKEN` | Token HuggingFace | - |
| `HF_MODEL` | Modelo HuggingFace | `microsoft/DialoGPT-small` |
| `BOT_SYSTEM_PROMPT` | Persona do bot | Assistente amigÃ¡vel PT-BR |
| `MEMORY_MAX_MESSAGES` | Mensagens no histÃ³rico | `10` |
| `USE_SQLITE` | Persistir conversas em SQLite | `false` |
| `DEBUG` | Ativar logs detalhados | `false` |

## â–¶ï¸ Como Executar

### Iniciar o servidor

```bash
# Com uvicorn (recomendado)
uvicorn app.main:app --reload

# Ou diretamente
python -m app.main
```

O servidor iniciarÃ¡ em `http://localhost:8000`.

### Verificar se estÃ¡ funcionando

Acesse no navegador:
- **Interface de Testes**: http://localhost:8000
- **DocumentaÃ§Ã£o interativa**: http://localhost:8000/docs
- **Health check**: http://localhost:8000/health

## ğŸ–¥ï¸ Interface Web (Frontend)

O projeto inclui uma interface web de testes integrada, acessÃ­vel em `http://localhost:8000` quando o servidor estÃ¡ rodando.

### Recursos

- **Design moderno escuro** com animaÃ§Ãµes suaves
- **Indicador de status** do modelo LLM em tempo real
- **HistÃ³rico de mensagens** por sessÃ£o
- **BotÃµes de aÃ§Ã£o rÃ¡pida** para testar prompts comuns
- **Gerenciamento de sessÃ£o** (nova sessÃ£o, limpar chat)

### Como usar

1. Inicie o servidor:
   ```bash
   uvicorn app.main:app --reload
   ```

2. Acesse no navegador: **http://localhost:8000**

3. Digite sua mensagem e pressione Enter ou clique no botÃ£o de enviar

> **Dica**: Use os botÃµes de aÃ§Ã£o rÃ¡pida para testar diferentes tipos de perguntas.

## ğŸ’¬ Exemplos de Uso

### Health Check

```bash
curl http://localhost:8000/health
```

Resposta:
```json
{
  "status": "healthy",
  "provider": "ollama",
  "model": "qwen2.5:0.5b",
  "provider_available": true,
  "message": null
}
```

### Enviar Mensagem

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "usuario-123",
    "message": "OlÃ¡, tudo bem?"
  }'
```

Resposta:
```json
{
  "session_id": "usuario-123",
  "reply": "OlÃ¡! Tudo bem sim, obrigado por perguntar! Como posso ajudar vocÃª hoje?",
  "provider": "ollama",
  "model": "qwen2.5:0.5b"
}
```

### Continuar Conversa (mesmo session_id)

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "usuario-123",
    "message": "Me explique o que Ã© Python"
  }'
```

O bot lembrarÃ¡ da conversa anterior porque usamos o mesmo `session_id`.

### Trocar Persona

No arquivo `.env`:
```env
BOT_SYSTEM_PROMPT=VocÃª Ã© um professor de programaÃ§Ã£o paciente e didÃ¡tico. Explique conceitos de forma simples, com exemplos prÃ¡ticos.
```

## ğŸ“ Estrutura de Pastas

```
chatbot-generico/
â”œâ”€â”€ app/                           # CÃ³digo da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # Entry point FastAPI
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py              # ConfiguraÃ§Ãµes (Pydantic Settings)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py             # Schemas request/response
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_provider.py        # Providers LLM (Ollama, HuggingFace)
â”‚   â”‚   â””â”€â”€ memory.py              # Gerenciador de memÃ³ria
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ routes.py              # Endpoints /chat, /health
â”œâ”€â”€ tests/                         # Testes automatizados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                # Fixtures pytest
â”‚   â””â”€â”€ test_api.py                # Testes dos endpoints
â”œâ”€â”€ .env.example                   # Template de configuraÃ§Ã£o
â”œâ”€â”€ pyproject.toml                 # DependÃªncias (Poetry/setuptools)
â”œâ”€â”€ requirements.txt               # DependÃªncias (pip)
â””â”€â”€ README.md                      # Este arquivo
```

## ğŸ“š API Reference

### POST /chat

Envia uma mensagem e recebe a resposta do chatbot.

**Request Body:**
```json
{
  "session_id": "string (1-100 chars)",
  "message": "string (1-4000 chars)"
}
```

**Response:**
```json
{
  "session_id": "string",
  "reply": "string",
  "provider": "ollama | huggingface",
  "model": "string"
}
```

**CÃ³digos de Status:**
- `200`: Sucesso
- `422`: Erro de validaÃ§Ã£o
- `503`: Provider nÃ£o disponÃ­vel

### GET /health

Verifica o status da aplicaÃ§Ã£o.

**Response:**
```json
{
  "status": "healthy | degraded | unhealthy",
  "provider": "string",
  "model": "string",
  "provider_available": true | false,
  "message": "string | null"
}
```

### GET /docs

DocumentaÃ§Ã£o interativa (Swagger UI).

### GET /redoc

DocumentaÃ§Ã£o alternativa (ReDoc).

## ğŸ§ª Testes

Execute os testes com pytest:

```bash
# Rodar todos os testes
pytest

# Com output verboso
pytest -v

# Com cobertura
pytest --cov=app
```

Os testes usam mocks para nÃ£o depender de Ollama/HuggingFace rodando.

## âš ï¸ LimitaÃ§Ãµes

1. **Modelo pequeno**: O Qwen 0.5B Ã© limitado em raciocÃ­nio complexo e pode dar respostas genÃ©ricas
2. **Sem RAG**: NÃ£o hÃ¡ integraÃ§Ã£o com documentos externos
3. **MemÃ³ria simples**: O histÃ³rico Ã© apenas concatenado, sem sumarizaÃ§Ã£o
4. **Sem autenticaÃ§Ã£o**: A API Ã© aberta (adicione auth para produÃ§Ã£o)
5. **Single-tenant**: NÃ£o hÃ¡ isolamento entre usuÃ¡rios
6. **CPU-only**: Modelos rodam em CPU (GPU acelera significativamente)

## ğŸš€ PrÃ³ximos Passos

SugestÃµes para evoluir o projeto:

1. **Frontend**: Criar interface web com React ou Vue
2. **AutenticaÃ§Ã£o**: Adicionar JWT ou API keys
3. **RAG**: Integrar com documentos usando embeddings
4. **Modelo maior**: Usar phi3:mini ou llama3.2:3b para melhor qualidade
5. **Cache**: Adicionar Redis para respostas frequentes
6. **Logs estruturados**: Usar Loguru ou structlog
7. **Docker**: Containerizar a aplicaÃ§Ã£o
8. **GPU**: Configurar CUDA/Metal para aceleraÃ§Ã£o

## ğŸ“„ LicenÃ§a

MIT License - use livremente para seu projeto de faculdade! ğŸ“

---

**Desenvolvido para fins educacionais** ğŸ“š

Se tiver dÃºvidas, abra uma issue ou consulte a documentaÃ§Ã£o em `/docs`.
